#import "/_settings/typst/template-note.typ": conf
#show: doc => conf(
  title: [
    Perceptrons
  ],
  lesson: "ARN",
  chapter: "2 - Perceptrons",
  definition: "",
  col: 1,
  doc,
)

= Perceptrons

#table(
  align: center + horizon,
  columns: (1fr, 1fr, 1fr),
  [*Caract√©ristique*], [*Ordinateur von Neumann*], [*Cerveau*],
  "Processeur", "Complexe (GHz)", "Simple (Hz) ",
  "Fr√©quence", "√âlev√©e", "Basse",
  "Nombre d'unit√©s", "Quelques c≈ìurs", "Tr√®s nombreuses",
  "M√©moire", "S√©par√©e, adressable", "Int√©gr√©e, distribu√©e",
  "Calcul", "Centralis√©, s√©quentiel", "Distribu√©, parall√®le",
  "Programmation", "Programmable", "Apprentissage",
  "Fiabilit√©", "Vuln√©rable", "Tr√®s robuste"
)

== Connexionnisme
- Mod√©lisation des ph√©nom√®nes mentaux/comportementaux comme des processus √©mergents de r√©seaux interconnect√©s.
- Architecture massivement parall√®le : chaque neurone corr√®le les entr√©es avec des poids synaptiques.
- Changement de paradigme #sym.arrow remplace la programmation par l'apprentissage.

#image("../img/image.png")

== Neurone Artificiel
Le neurone artificiel, invent√© par Warren McCulloch et Walter Pitts en 1948, est une unit√© de calcul qui prend des entr√©es, les pond√®re et les somme pour produire une sortie.

#image("../img/image copy.png")

=== Formule
Si (x1 + x2 + ... + xn) ‚â• Œò alors y = 1, sinon y = 0

- Entr√©es : excitatrices (poids = 1) et inhibitrices (poids = -1).
- Sortie binaire.

== Perceptron de Rosenblatt (1958)

#columns(2)[
  #linebreak()
Si $sum w_i x_i gt.eq Theta$ alors y = 1, sinon y = -1

- Somme pond√©r√©e des entr√©es.
- Fonction d'activation : discontinue.
#colbreak()
#image("../img/image copy 2.png")
]


== S√©paration Lin√©aire
- Le perceptron s√©pare les donn√©es par un hyperplan :
  - 2D : une ligne.
  - 3D : un plan.
  - nD : un hyperplan.

== Biais
- Se d√©barrasse du seuil en ajoutant un poids suppl√©mentaire appel√© "biais" :

Si ‚àë wixi + w0 ‚â• 0 alors y = 1, sinon y = 0


== Algorithme d'Apprentissage du Perceptron
1. Initialisation al√©atoire des poids.
2. Calcul de la sortie `y` pour une entr√©e `x`.
3. Mise √† jour des poids :

Wj(t+1) = Wj(t) + Œ∑(d - y)xj

4. R√©p√©ter jusqu'√† ce que l'erreur soit inf√©rieure √† un seuil.

== Descente de Gradient (Widrow-Hoff / Delta Rule)
- Minimise la fonction d'erreur :

E = 1/2 ‚àë (yi - di)¬≤

- Mise √† jour des poids :

Œîwj = Œ∑(d - y)xj


== Fonction d'Activation Sigmo√Øde

y = 1 / (1 + e^(-Sj))

- Activation non lin√©aire souvent utilis√©e.
- Autres fonctions courantes : tanh, ReLU.

== Classification √† Deux Classes
- Le perceptron classe les donn√©es selon un seuil fix√©.
- √âquilibre entre taux de vrais positifs et taux de faux positifs.

== Probl√®mes Non-Lin√©airement S√©parables
- Le perceptron ne peut pas s√©parer certaines classes avec une ligne simple.


Dis-moi si tu veux que j‚Äôajoute ou clarifie des parties sp√©cifiques ! üöÄ